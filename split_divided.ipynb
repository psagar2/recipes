{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psagar2/recipes/blob/main/split_divided.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction:\n",
        "As per the World Health Organisation, each year approximately 1.3 million people lose their lives because of road accidents.[2] Along with drunk driving and unconsciousness, the major reason that seems to be contributing is distracted driving.\n",
        "Distracted driver detection is a major contributor towards road accidents these days and accounting as public health threat these days.[1] For solving this problem, a video extracted images of distracted drivers’ dataset is used which is easily accessible on Kaggle. A model for real-time detection for distracted drivers and notifying them is required. Moreover, if this is combined with automated driving which is currently pursued by TESLA, will make this world a safer place to drive, and the number of accidents will reduce. It is quite well accepted that computer vision majorly gives the optimum results for image classification.  Hence for this distracted driver detection, CNN was applied.\n"
      ],
      "metadata": {
        "id": "uaDC4Q_OVDEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Literature Review:\n",
        "In previous years Distracted driver detection has been a topic that has gathered the attention of many scientists. The work started in the early 90s when the mobile phone started to come into the scenario. Since then, usage of mobile while driving is one of the major contributors to the causes of distracted drivers [1]. With the advancement of technology, this started to increase as the users are increasing and dependence on smartphones is increasing day by day.¬¬ Many researchers have worked on this by making their datasets and applying models to them. Deep neural networks and convolution neural networks (CNN) have been used in the majority of the works till now. Some recent works include the model of ensemble learning for the prediction using video feed this was achieved by using CNN and combining it with Long Short-Term Memory which is part of RNN. This used sensor data along with an image to achieve ensemble learning [2]. "
      ],
      "metadata": {
        "id": "o3UsxsNWVJTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregation of CNN and Histogram of Gradients (HOG) has also been used for classification as this uses CNN for image processing and HOG is used for detection of human beings as it collects small gradients, processes their connections, and then normalize to use them for prediction of the regions [3]. Most of these research’s use pre-trained models for processing the images and using them for predictions mostly used are VGG 16, AlexNet, ResNet-50, and Inception V3. These models are used to provide pre-trained weights and their outputs are then flattened to add to certain layers than at the end mapped to categories for output. The number of intermediate layers depends on the researcher. The pretrained model is then combined with Human activity recognition models to detect the features of the driver using K-means Clustering [4]. As the data of sensors and the video, the dataset is not publicly available, and the computational resources are restricted in this project usage of the image is done that too with the pixel size of 160X160. If better computational resources are present, it is recommended to use 256X256 size images.\n",
        "\n"
      ],
      "metadata": {
        "id": "atX_RV_AVYNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing json file and unzipping the data"
      ],
      "metadata": {
        "id": "UmmqHg19QrVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIHIQlrCF1O9"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c state-farm-distracted-driver-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeC8AZveGBWC"
      },
      "outputs": [],
      "source": [
        "! mkdir driverData\n",
        "! unzip driverData.zip -d driverData"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading libraries"
      ],
      "metadata": {
        "id": "sOpCC9MY_UBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandasql"
      ],
      "metadata": {
        "id": "sR3SwI5_7gRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "12My0arRGDXS"
      },
      "outputs": [],
      "source": [
        "#loading the libraries\n",
        "#for the usage of array:\n",
        "import numpy as np \n",
        "#for impementing dataset and operations:\n",
        "import pandasql as ps\n",
        "import pandas as pd\n",
        "#for traversing in the dataset:\n",
        "import os\n",
        "#to convert data in pickle files for reproducibility:\n",
        "import pickle\n",
        "#for applying computer vision on images:\n",
        "import cv2\n",
        "#to use image pre processing:\n",
        "from keras.preprocessing import image\n",
        "#to apply models\n",
        "import tensorflow as tf\n",
        "#to plot the charts\n",
        "import matplotlib.pyplot as plt\n",
        "#for applying machine models and layers\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data pre-processing:\n",
        "Dataset consists of 22426 images divided into 10 categories namely:\n",
        "1.\tSafe driving \n",
        "2.\tTalking on the phone right\n",
        "3.\tTalking on the phone left\n",
        "4.\tLooking back \n",
        "5.\tTalking to the passenger \n",
        "6.\tDrinking \n",
        "7.\tTexting right\n",
        "8.\tTexting left\n",
        "9.\tHair and makeup \n",
        "10.\tOperating the radio"
      ],
      "metadata": {
        "id": "Zu1147tEVdey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The major challenge with this dataset is that it has quite similar images that confuse the model to easily determine the difference. Therefore, image augmentation is quite necessary for this dataset. \n",
        "Image augmentation types:\n",
        "1. Horizontal flip: flips random images horizontally from the dataset.\n",
        "2. Vertical flip: flips random images vertically from the dataset\n",
        "3. Rotation range: randomly rotates images inside the dataset so our model can be robust.\n",
        "4. Validation split: 0.3 that is 30% data split.\n",
        "5. Fill mode=’nearest’:\n",
        "6. Zoom range: randomly zooming images by 20%\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "miabhOo8VpxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, these augmentations have been tried at different stages with different values and the values mentioned above are giving the best results. These augmentations have been applied inside through computer vision library CV2 and image data generator class, varying with the way data was considered.\n",
        "There were 3 approaches considered on the dataset, those are providing the data through google drive and doing a train test split through a Keras function, taking the data from the drive and partitioning it based on people out of the 26 being considered for the study and the last one is partitioning through image data generator class. First, the Train test split was done by taking 30% as validation data and 70% as training data. Second, the Train test split based on people: as 26 participants have accounted for 22424 images, we randomly chose 6 people and added them to the test set so that training data and test data could be different."
      ],
      "metadata": {
        "id": "iQTpqe0rV4KC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input shapes: (32,32,3), (64,64,3), (128,128,3) and (128,128,1). Images were imported on different pixel sizes and accuracy was drastically dropping in lower pixel sizes.  At 128 X 128 RGB to Gray was also considered to see if that enables models to get better accuracy with a lower number of hyperparameters and, hence, saves time and speed of computation."
      ],
      "metadata": {
        "id": "IMtW_7WkV706"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resizing of image: all the images are converted to NumPy array and then resized by 1/255 as the images have to be normalized to apply the image analysis. We can decide the batch size so that we can define how many photos are present in each batch. That is required as batch normalization needs to be applied which we will be discussed later in the report.\n"
      ],
      "metadata": {
        "id": "LVZMpYdZV_eL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqysAoAjGOV6",
        "outputId": "477eaed9-86ff-495e-8c7b-c81883781fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22424 images belonging to 10 classes.\n",
            "Found 6722 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# Image augmentation and Train test split\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   validation_split=0.3,\n",
        "                                   rotation_range=180,\n",
        "                                   #shear_range=0.2,\n",
        "                                   zoom_range=0.5,\n",
        "                                   #horizontal_flip=True,\n",
        "                                   #vertical_flip=True,\n",
        "                                   fill_mode='nearest',\n",
        "                                   #featurewise_center=True,\n",
        "                                   ) # set validation split\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/driverData/imgs/train/',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=128,\n",
        "    ) # set as training data\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    '/content/driverData/imgs/train/',  # same directory as training data\n",
        "    target_size=(128, 128),\n",
        "    batch_size=128,\n",
        "    subset='validation') # set as validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory data analysis:\n",
        "There are certain questions that how the data is distributed and how is it justifiable to consider the data? \n",
        "So, to answer these questions the distribution of data was considered, how many photos of each person are present is taken into account and the uniformity with categories is checked.\n",
        "How many photos of each category are present? \n"
      ],
      "metadata": {
        "id": "nWWNUSc0WXEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1=pd.read_csv('/content/driverData/driver_imgs_list.csv')"
      ],
      "metadata": {
        "id": "F63WWrWR7fay"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=ps.sqldf('select classname, count(img) as Number_of_images from df1 group by classname')"
      ],
      "metadata": {
        "id": "_479riw_Wecf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(df2.classname, df2.Number_of_images)\n",
        "plt.xlabel(\"Categories\")\n",
        "plt.ylabel(\"No. of Images\")"
      ],
      "metadata": {
        "id": "bxwEB5aCWoWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the data suggests there is an almost equal number of images in each category. That suggests the model can be easily trained on these images as no particular category would dominate others.\n",
        "\n",
        "How many pictures of each driver are present? \n"
      ],
      "metadata": {
        "id": "PcLUjgVkXZzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3=ps.sqldf('select subject, count(img) as Number_of_images from df1 group by subject')\n",
        "df3"
      ],
      "metadata": {
        "id": "5pEaQ6KKXkm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(df3.subject, df3.Number_of_images)\n",
        "plt.xlabel(\"subject\")\n",
        "plt.ylabel(\"No. of Images\")\n",
        "plt.autoscale"
      ],
      "metadata": {
        "id": "jbm-HG_9Wv8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was needed to be checked as if one particular driver dominates the whole dataset there are chances that the model would start classifying based on that driver’s features."
      ],
      "metadata": {
        "id": "BwMIdF_kWn2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input and Output Layers components:\n",
        "1.\tConv 2D: convolution layer of 2 dimensions is run over an image using a spatial matrix which uses filters which are values of the 2 dimensions, strides these are the number of pixels to skip by the filter, kernel size is the filter size for the image.\n",
        "2.\tMax pooling:  CNN works by extracting the features from the image. For example, in the texting right category, we have a mobile phone in the right hand as one of the features. Now as the spatial matrix traverse over the image to learn it might learn some features as it is. So the model will not predict if it is rotated or present in a different scenario. Down sampling that is reducing the resolution is required. Hence max pooling is one of the types of down sampling which considers the matrix of a certain size chooses the maximum value out of those pixels and replaces it instead.\n",
        "3.\tDense: dense layer is a fully connected layer to its preceding layer and it takes all the neurons information from the previous layer and adds bias to the value. Then the value is passed to the activation function which determines the value of this node.\n",
        "4.\tDropout: dropout layer works with setting the neurons input values with 0. It doesn’t let the model overfit and replaces the values with 0 and change the other values with 1/(1-rate) such that the sum of all inputs is unchanged.\n",
        "5.\tBatch normalization: batch normalization works to maintain mean output as 0 and works to maintain standard deviation of output at 1. This is a transformation that works in 2 different ways when applied to training or testing samples for training it needs to be applied during fit function and for testing it is applied at evaluation.\n",
        "6.\tFilter size: the filter size works to extract the features as the filters move over an image it returns the value by multiplying an n-dimensional matrix which only has the values over the diagonal and those values are 1. It extracts those features and returns the value. The filter size that is the value of n depends on the training data and type of features.\n",
        "7.\tInput shape: the input shape depends on the images’ pixel size, the features needed for classification, and the available computational resources. \n",
        "8.\tActivation functions:\n",
        "    8.1.\tTanh: tanh function is straight from the trigonometry returns the value of (exp(x)-exp(-x)) /(exp(x)+exp(-x))\n",
        "    8.2.\tSigmoid: Sigmoid function returns the values between 0 and 1 it uses a function that considers the equation of 1/1+exp(-x). So, it returns the value nearby zero if the value of x is small and near to 1 if the value of X is big.\n",
        "    8.3.\tReLu: Rectified Linear unit is a function that returns the value of the maximum of 2 values in which the first is 0 and the second is x (the input vector). This returns all the positive values.\n",
        "    8.4.\tSwish: this is a fairly new activation function; it has not been included in the official Keras documentation till now. It is an extension of the sigmoid function that is it returns the value of x*sigmoid(x).\n",
        "    8.5.\tSoftMax: The SoftMax function works on the idea of probability distribution and returns the output vector whose values are in the range of 0 and 1. Also, the sum of these output vectors is 1. This is majorly used in the last layer of the model for classification purposes.\n",
        "9.\tOptimizers used:\n",
        "    9.1.\t SGD: stochastic gradient descent is an originally robust model that shapes most optimizers today. Sgd progresses a bit slow. Its learning rate is 0.01.\n",
        "    9.2.\t ADAM: Adam is the method that is based on the adaptive estimation of stochastic gradient descent. It is the most used optimizer. Its learning rate is 0.001.\n"
      ],
      "metadata": {
        "id": "Y2fBfJEEYVx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology:\n",
        "The model chosen here is sequential, the first convolution layer has 64 nodes and a 16X16 filter for adequate feature attraction. The activation function used for the first layer is swish. Max pooling filter of 2X2 used here for a down sampling. After this, a dropout layer is introduced with a 0.2 value that is 20 percent of the nodes are replaced with 0, and the rest are multiplied by 1/0.8. After that, nodes are increased to 128 so that the features can be further divided and processed again the same filter of 16X16 is passed to analyze and activation functions are kept the same. Here max-pooling of 4X4 is added with strides value as 2 which means this will skip 1 in between for down sampling. Again, a dropout of 20% is used. After this, the output is flattened and mapped with a hidden layer of 1024 units where activation function ReLu is used. Then the next hidden layer concise the size to converge the learning of the nodes and drive towards the conclusion hence has 256 nodes with a dropout of 0.2. The next layer has 128 nodes with activation function swish. Then dropout with 0.2 is introduced here which finally maps with the output layer which returns a predicted value which is one hot encoded and this is then deciphered to know the predicted category and check the accuracy.\n"
      ],
      "metadata": {
        "id": "RXbUBQehY9rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model12=Sequential()\n",
        "model12.add(Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(128,128,3)))\n",
        "model12.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model12.add(Dropout(0.2))\n",
        "model12.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "model12.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model12.add(Dropout(0.2))\n",
        "model12.add(Flatten())\n",
        "model12.add(Dense(128,activation='relu'))\n",
        "model12.add(Dropout(0.5))\n",
        "model12.add(Dense(64,activation='relu'))\n",
        "model12.add(Dense(10, activation='softmax'))\n",
        "#tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model12.summary()\n",
        "model12.compile(loss='categorical_crossentropy',optimizer='Adam', metrics=['accuracy'])\n",
        "hist12=model12.fit(train_generator,epochs=50,validation_data=(validation_generator), verbose=1)\n",
        "print(\"Model Training Complete...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu8LiVg1ZAHr",
        "outputId": "c0243e16-39e4-462e-b64c-c85ccaffeb34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 128, 128, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 64, 64, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 32, 32, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 131072)            0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               16777344  \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,861,898\n",
            "Trainable params: 16,861,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "176/176 [==============================] - 231s 1s/step - loss: 2.3460 - accuracy: 0.1081 - val_loss: 2.2964 - val_accuracy: 0.1089\n",
            "Epoch 2/50\n",
            "176/176 [==============================] - 230s 1s/step - loss: 2.2974 - accuracy: 0.1107 - val_loss: 2.2870 - val_accuracy: 0.1187\n",
            "Epoch 3/50\n",
            "176/176 [==============================] - 233s 1s/step - loss: 2.2905 - accuracy: 0.1238 - val_loss: 2.2749 - val_accuracy: 0.1323\n",
            "Epoch 4/50\n",
            "176/176 [==============================] - 234s 1s/step - loss: 2.2691 - accuracy: 0.1373 - val_loss: 2.2164 - val_accuracy: 0.1809\n",
            "Epoch 5/50\n",
            "176/176 [==============================] - 226s 1s/step - loss: 2.1949 - accuracy: 0.1852 - val_loss: 2.0920 - val_accuracy: 0.2426\n",
            "Epoch 6/50\n",
            "176/176 [==============================] - 226s 1s/step - loss: 2.0724 - accuracy: 0.2341 - val_loss: 1.9597 - val_accuracy: 0.3057\n",
            "Epoch 7/50\n",
            "176/176 [==============================] - 226s 1s/step - loss: 1.9632 - accuracy: 0.2809 - val_loss: 1.8117 - val_accuracy: 0.3307\n",
            "Epoch 8/50\n",
            "176/176 [==============================] - 227s 1s/step - loss: 1.8663 - accuracy: 0.3061 - val_loss: 1.7032 - val_accuracy: 0.3831\n",
            "Epoch 9/50\n",
            "176/176 [==============================] - 230s 1s/step - loss: 1.7971 - accuracy: 0.3303 - val_loss: 1.6392 - val_accuracy: 0.3957\n",
            "Epoch 10/50\n",
            "176/176 [==============================] - 230s 1s/step - loss: 1.7385 - accuracy: 0.3483 - val_loss: 1.5753 - val_accuracy: 0.4122\n",
            "Epoch 11/50\n",
            "176/176 [==============================] - 229s 1s/step - loss: 1.6883 - accuracy: 0.3647 - val_loss: 1.4942 - val_accuracy: 0.4518\n",
            "Epoch 12/50\n",
            "176/176 [==============================] - 230s 1s/step - loss: 1.6391 - accuracy: 0.3773 - val_loss: 1.4654 - val_accuracy: 0.4439\n",
            "Epoch 13/50\n",
            "176/176 [==============================] - 229s 1s/step - loss: 1.6138 - accuracy: 0.3888 - val_loss: 1.4334 - val_accuracy: 0.4622\n",
            "Epoch 14/50\n",
            "176/176 [==============================] - 229s 1s/step - loss: 1.5995 - accuracy: 0.3975 - val_loss: 1.3915 - val_accuracy: 0.4740\n",
            "Epoch 15/50\n",
            "176/176 [==============================] - 230s 1s/step - loss: 1.5665 - accuracy: 0.4072 - val_loss: 1.3447 - val_accuracy: 0.4949\n",
            "Epoch 16/50\n",
            "176/176 [==============================] - 229s 1s/step - loss: 1.5262 - accuracy: 0.4196 - val_loss: 1.3280 - val_accuracy: 0.5128\n",
            "Epoch 17/50\n",
            "176/176 [==============================] - 229s 1s/step - loss: 1.5129 - accuracy: 0.4282 - val_loss: 1.3008 - val_accuracy: 0.5171\n",
            "Epoch 18/50\n",
            "176/176 [==============================] - 231s 1s/step - loss: 1.4992 - accuracy: 0.4353 - val_loss: 1.2972 - val_accuracy: 0.5260\n",
            "Epoch 19/50\n",
            "176/176 [==============================] - 229s 1s/step - loss: 1.4775 - accuracy: 0.4424 - val_loss: 1.2476 - val_accuracy: 0.5379\n",
            "Epoch 20/50\n",
            "176/176 [==============================] - 229s 1s/step - loss: 1.4464 - accuracy: 0.4510 - val_loss: 1.2413 - val_accuracy: 0.5519\n",
            "Epoch 21/50\n",
            "176/176 [==============================] - 232s 1s/step - loss: 1.4268 - accuracy: 0.4556 - val_loss: 1.2199 - val_accuracy: 0.5733\n",
            "Epoch 22/50\n",
            "176/176 [==============================] - 236s 1s/step - loss: 1.4213 - accuracy: 0.4601 - val_loss: 1.2519 - val_accuracy: 0.5425\n",
            "Epoch 23/50\n",
            "176/176 [==============================] - 227s 1s/step - loss: 1.3993 - accuracy: 0.4709 - val_loss: 1.1835 - val_accuracy: 0.5698\n",
            "Epoch 24/50\n",
            "176/176 [==============================] - 225s 1s/step - loss: 1.3769 - accuracy: 0.4821 - val_loss: 1.1543 - val_accuracy: 0.5887\n",
            "Epoch 25/50\n",
            "176/176 [==============================] - 225s 1s/step - loss: 1.3590 - accuracy: 0.4905 - val_loss: 1.1283 - val_accuracy: 0.6019\n",
            "Epoch 26/50\n",
            "176/176 [==============================] - 223s 1s/step - loss: 1.3377 - accuracy: 0.4988 - val_loss: 1.1154 - val_accuracy: 0.6025\n",
            "Epoch 27/50\n",
            "176/176 [==============================] - 223s 1s/step - loss: 1.3262 - accuracy: 0.5023 - val_loss: 1.1330 - val_accuracy: 0.5955\n",
            "Epoch 28/50\n",
            "176/176 [==============================] - 224s 1s/step - loss: 1.3191 - accuracy: 0.5050 - val_loss: 1.0892 - val_accuracy: 0.6125\n",
            "Epoch 29/50\n",
            "176/176 [==============================] - 224s 1s/step - loss: 1.2980 - accuracy: 0.5134 - val_loss: 1.0619 - val_accuracy: 0.6360\n",
            "Epoch 30/50\n",
            "176/176 [==============================] - 226s 1s/step - loss: 1.2899 - accuracy: 0.5199 - val_loss: 1.0356 - val_accuracy: 0.6323\n",
            "Epoch 31/50\n",
            "176/176 [==============================] - 226s 1s/step - loss: 1.2666 - accuracy: 0.5301 - val_loss: 1.0260 - val_accuracy: 0.6459\n",
            "Epoch 32/50\n",
            "176/176 [==============================] - 227s 1s/step - loss: 1.2555 - accuracy: 0.5300 - val_loss: 1.0063 - val_accuracy: 0.6506\n",
            "Epoch 33/50\n",
            "176/176 [==============================] - 228s 1s/step - loss: 1.2389 - accuracy: 0.5375 - val_loss: 1.0109 - val_accuracy: 0.6455\n",
            "Epoch 34/50\n",
            "176/176 [==============================] - 228s 1s/step - loss: 1.2256 - accuracy: 0.5425 - val_loss: 1.0000 - val_accuracy: 0.6513\n",
            "Epoch 35/50\n",
            "176/176 [==============================] - 228s 1s/step - loss: 1.2130 - accuracy: 0.5471 - val_loss: 0.9425 - val_accuracy: 0.6688\n",
            "Epoch 36/50\n",
            "176/176 [==============================] - 229s 1s/step - loss: 1.1946 - accuracy: 0.5573 - val_loss: 0.9492 - val_accuracy: 0.6714\n",
            "Epoch 37/50\n",
            "176/176 [==============================] - 228s 1s/step - loss: 1.1778 - accuracy: 0.5611 - val_loss: 0.9083 - val_accuracy: 0.6855\n",
            "Epoch 38/50\n",
            "176/176 [==============================] - 228s 1s/step - loss: 1.1690 - accuracy: 0.5645 - val_loss: 0.8962 - val_accuracy: 0.6897\n",
            "Epoch 39/50\n",
            "176/176 [==============================] - 228s 1s/step - loss: 1.1666 - accuracy: 0.5686 - val_loss: 0.8903 - val_accuracy: 0.6900\n",
            "Epoch 40/50\n",
            "176/176 [==============================] - 227s 1s/step - loss: 1.1531 - accuracy: 0.5704 - val_loss: 0.8705 - val_accuracy: 0.7078\n",
            "Epoch 41/50\n",
            "176/176 [==============================] - 225s 1s/step - loss: 1.1409 - accuracy: 0.5781 - val_loss: 0.8759 - val_accuracy: 0.6999\n",
            "Epoch 42/50\n",
            "176/176 [==============================] - 228s 1s/step - loss: 1.1300 - accuracy: 0.5801 - val_loss: 0.8446 - val_accuracy: 0.7133\n",
            "Epoch 43/50\n",
            "176/176 [==============================] - 225s 1s/step - loss: 1.1083 - accuracy: 0.5881 - val_loss: 0.8575 - val_accuracy: 0.7017\n",
            "Epoch 44/50\n",
            "176/176 [==============================] - 227s 1s/step - loss: 1.1167 - accuracy: 0.5894 - val_loss: 0.8390 - val_accuracy: 0.7077\n",
            "Epoch 45/50\n",
            "176/176 [==============================] - 230s 1s/step - loss: 1.0975 - accuracy: 0.5934 - val_loss: 0.8325 - val_accuracy: 0.7270\n",
            "Epoch 46/50\n",
            "176/176 [==============================] - 224s 1s/step - loss: 1.0960 - accuracy: 0.5956 - val_loss: 0.8094 - val_accuracy: 0.7205\n",
            "Epoch 47/50\n",
            "176/176 [==============================] - 223s 1s/step - loss: 1.0787 - accuracy: 0.6052 - val_loss: 0.8153 - val_accuracy: 0.7246\n",
            "Epoch 48/50\n",
            "176/176 [==============================] - 223s 1s/step - loss: 1.0753 - accuracy: 0.6038 - val_loss: 0.8077 - val_accuracy: 0.7233\n",
            "Epoch 49/50\n",
            "176/176 [==============================] - 224s 1s/step - loss: 1.0638 - accuracy: 0.6118 - val_loss: 0.7927 - val_accuracy: 0.7217\n",
            "Epoch 50/50\n",
            "176/176 [==============================] - 225s 1s/step - loss: 1.0564 - accuracy: 0.6106 - val_loss: 0.7744 - val_accuracy: 0.7395\n",
            "Model Training Complete...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L95-YVlHGaVP",
        "outputId": "b692c784-f108-4f94-88d2-01cc8d31b5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 160, 160, 64)      49216     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 80, 80, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 80, 80, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 80, 80, 128)       2097280   \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 39, 39, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 39, 39, 128)       0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 194688)            0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              199361536 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               262400    \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 201,804,618\n",
            "Trainable params: 201,804,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "176/176 [==============================] - 281s 1s/step - loss: 2.3004 - accuracy: 0.1093 - val_loss: 2.2991 - val_accuracy: 0.1110\n",
            "Epoch 2/50\n",
            "176/176 [==============================] - 249s 1s/step - loss: 2.2988 - accuracy: 0.1143 - val_loss: 2.2976 - val_accuracy: 0.1156\n",
            "Epoch 3/50\n",
            "176/176 [==============================] - 246s 1s/step - loss: 2.2973 - accuracy: 0.1195 - val_loss: 2.2948 - val_accuracy: 0.1500\n",
            "Epoch 4/50\n",
            "176/176 [==============================] - 247s 1s/step - loss: 2.2936 - accuracy: 0.1259 - val_loss: 2.2895 - val_accuracy: 0.1287\n",
            "Epoch 5/50\n",
            "176/176 [==============================] - 246s 1s/step - loss: 2.2856 - accuracy: 0.1365 - val_loss: 2.2755 - val_accuracy: 0.1581\n",
            "Epoch 6/50\n",
            "176/176 [==============================] - 248s 1s/step - loss: 2.2628 - accuracy: 0.1616 - val_loss: 2.2430 - val_accuracy: 0.1729\n",
            "Epoch 7/50\n",
            "176/176 [==============================] - 248s 1s/step - loss: 2.2300 - accuracy: 0.1823 - val_loss: 2.2091 - val_accuracy: 0.1979\n",
            "Epoch 8/50\n",
            "176/176 [==============================] - 249s 1s/step - loss: 2.1908 - accuracy: 0.2020 - val_loss: 2.1495 - val_accuracy: 0.2223\n",
            "Epoch 9/50\n",
            "176/176 [==============================] - 249s 1s/step - loss: 2.1355 - accuracy: 0.2243 - val_loss: 2.0857 - val_accuracy: 0.2705\n",
            "Epoch 10/50\n",
            "176/176 [==============================] - 249s 1s/step - loss: 2.0433 - accuracy: 0.2689 - val_loss: 1.9434 - val_accuracy: 0.3166\n",
            "Epoch 11/50\n",
            "176/176 [==============================] - 250s 1s/step - loss: 1.9127 - accuracy: 0.3181 - val_loss: 1.8167 - val_accuracy: 0.3469\n",
            "Epoch 12/50\n",
            "176/176 [==============================] - 251s 1s/step - loss: 1.7697 - accuracy: 0.3613 - val_loss: 1.6277 - val_accuracy: 0.4261\n",
            "Epoch 13/50\n",
            "176/176 [==============================] - 249s 1s/step - loss: 1.6284 - accuracy: 0.4101 - val_loss: 1.4692 - val_accuracy: 0.4763\n",
            "Epoch 14/50\n",
            "176/176 [==============================] - 249s 1s/step - loss: 1.5020 - accuracy: 0.4595 - val_loss: 1.3440 - val_accuracy: 0.5266\n",
            "Epoch 15/50\n",
            "176/176 [==============================] - 252s 1s/step - loss: 1.3962 - accuracy: 0.4949 - val_loss: 1.2495 - val_accuracy: 0.5608\n",
            "Epoch 16/50\n",
            "176/176 [==============================] - 249s 1s/step - loss: 1.2890 - accuracy: 0.5342 - val_loss: 1.1004 - val_accuracy: 0.6151\n",
            "Epoch 17/50\n",
            "176/176 [==============================] - 253s 1s/step - loss: 1.2042 - accuracy: 0.5690 - val_loss: 1.0203 - val_accuracy: 0.6528\n",
            "Epoch 18/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 1.1233 - accuracy: 0.5977 - val_loss: 0.9585 - val_accuracy: 0.6669\n",
            "Epoch 19/50\n",
            "176/176 [==============================] - 257s 1s/step - loss: 1.0584 - accuracy: 0.6208 - val_loss: 0.8837 - val_accuracy: 0.7062\n",
            "Epoch 20/50\n",
            "176/176 [==============================] - 257s 1s/step - loss: 0.9986 - accuracy: 0.6445 - val_loss: 0.8159 - val_accuracy: 0.7272\n",
            "Epoch 21/50\n",
            "176/176 [==============================] - 257s 1s/step - loss: 0.9270 - accuracy: 0.6731 - val_loss: 0.7480 - val_accuracy: 0.7505\n",
            "Epoch 22/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.8830 - accuracy: 0.6922 - val_loss: 0.7165 - val_accuracy: 0.7645\n",
            "Epoch 23/50\n",
            "176/176 [==============================] - 257s 1s/step - loss: 0.8359 - accuracy: 0.7104 - val_loss: 0.6594 - val_accuracy: 0.7832\n",
            "Epoch 24/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.7855 - accuracy: 0.7261 - val_loss: 0.6162 - val_accuracy: 0.7972\n",
            "Epoch 25/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.7416 - accuracy: 0.7461 - val_loss: 0.6112 - val_accuracy: 0.7953\n",
            "Epoch 26/50\n",
            "176/176 [==============================] - 255s 1s/step - loss: 0.6976 - accuracy: 0.7594 - val_loss: 0.5319 - val_accuracy: 0.8268\n",
            "Epoch 27/50\n",
            "176/176 [==============================] - 252s 1s/step - loss: 0.6845 - accuracy: 0.7648 - val_loss: 0.5272 - val_accuracy: 0.8331\n",
            "Epoch 28/50\n",
            "176/176 [==============================] - 251s 1s/step - loss: 0.6493 - accuracy: 0.7752 - val_loss: 0.5050 - val_accuracy: 0.8303\n",
            "Epoch 29/50\n",
            "176/176 [==============================] - 252s 1s/step - loss: 0.6232 - accuracy: 0.7873 - val_loss: 0.4819 - val_accuracy: 0.8423\n",
            "Epoch 30/50\n",
            "176/176 [==============================] - 251s 1s/step - loss: 0.6000 - accuracy: 0.7967 - val_loss: 0.4577 - val_accuracy: 0.8487\n",
            "Epoch 31/50\n",
            "176/176 [==============================] - 255s 1s/step - loss: 0.5714 - accuracy: 0.8070 - val_loss: 0.4210 - val_accuracy: 0.8631\n",
            "Epoch 32/50\n",
            "176/176 [==============================] - 255s 1s/step - loss: 0.5431 - accuracy: 0.8145 - val_loss: 0.4090 - val_accuracy: 0.8727\n",
            "Epoch 33/50\n",
            "176/176 [==============================] - 252s 1s/step - loss: 0.5267 - accuracy: 0.8218 - val_loss: 0.4035 - val_accuracy: 0.8682\n",
            "Epoch 34/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.5144 - accuracy: 0.8287 - val_loss: 0.3620 - val_accuracy: 0.8846\n",
            "Epoch 35/50\n",
            "176/176 [==============================] - 251s 1s/step - loss: 0.4882 - accuracy: 0.8389 - val_loss: 0.3766 - val_accuracy: 0.8777\n",
            "Epoch 36/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.4800 - accuracy: 0.8416 - val_loss: 0.3507 - val_accuracy: 0.8843\n",
            "Epoch 37/50\n",
            "176/176 [==============================] - 253s 1s/step - loss: 0.4612 - accuracy: 0.8436 - val_loss: 0.3361 - val_accuracy: 0.8979\n",
            "Epoch 38/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.4432 - accuracy: 0.8532 - val_loss: 0.3155 - val_accuracy: 0.8996\n",
            "Epoch 39/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.4241 - accuracy: 0.8607 - val_loss: 0.2996 - val_accuracy: 0.9064\n",
            "Epoch 40/50\n",
            "176/176 [==============================] - 255s 1s/step - loss: 0.4194 - accuracy: 0.8592 - val_loss: 0.2871 - val_accuracy: 0.9112\n",
            "Epoch 41/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.3984 - accuracy: 0.8659 - val_loss: 0.2861 - val_accuracy: 0.9087\n",
            "Epoch 42/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.3934 - accuracy: 0.8722 - val_loss: 0.2981 - val_accuracy: 0.9027\n",
            "Epoch 43/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.3806 - accuracy: 0.8743 - val_loss: 0.2843 - val_accuracy: 0.9090\n",
            "Epoch 44/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.3653 - accuracy: 0.8806 - val_loss: 0.2567 - val_accuracy: 0.9185\n",
            "Epoch 45/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.3605 - accuracy: 0.8810 - val_loss: 0.2549 - val_accuracy: 0.9226\n",
            "Epoch 46/50\n",
            "176/176 [==============================] - 256s 1s/step - loss: 0.3412 - accuracy: 0.8867 - val_loss: 0.2376 - val_accuracy: 0.9287\n",
            "Epoch 47/50\n",
            "176/176 [==============================] - 254s 1s/step - loss: 0.3344 - accuracy: 0.8910 - val_loss: 0.2506 - val_accuracy: 0.9191\n",
            "Epoch 48/50\n",
            "176/176 [==============================] - 255s 1s/step - loss: 0.3230 - accuracy: 0.8959 - val_loss: 0.2380 - val_accuracy: 0.9210\n",
            "Epoch 49/50\n",
            "176/176 [==============================] - 252s 1s/step - loss: 0.3180 - accuracy: 0.8960 - val_loss: 0.2438 - val_accuracy: 0.9241\n",
            "Epoch 50/50\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.3206 - accuracy: 0.8965 - val_loss: 0.2143 - val_accuracy: 0.9351\n",
            "Model Training Complete...\n"
          ]
        }
      ],
      "source": [
        "model6=Sequential()\n",
        "\n",
        "model6.add(Conv2D(64, (16,16), activation='swish', padding='same', input_shape=(160,160,3)))\n",
        "#model6.add(Conv2D(64, (4,4), activation='swish', padding='same'))\n",
        "model6.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
        "model6.add(Dropout(0.2))\n",
        "model6.add(Conv2D(128, (16,16), activation='swish', padding='same'))\n",
        "#model6.add(Conv2D(128, (4,4), activation='swish', padding='same'))\n",
        "model6.add(MaxPooling2D(pool_size=(4,4),strides=2))\n",
        "model6.add(Dropout(0.2))\n",
        "# model6.add(Conv2D(256, (4,4), activation='swish', padding='same'))\n",
        "# #model6.add(Conv2D(256, (4,4), activation='swish', padding='same'))\n",
        "# model6.add(MaxPooling2D(pool_size=(4,4),strides=2))\n",
        "# model6.add(Dropout(0.2))\n",
        "model6.add(Flatten())\n",
        "model6.add(Dense(1024,activation='relu'))\n",
        "model6.add(Dense(256,activation='swish'))\n",
        "model6.add(Dropout(0.2))\n",
        "model6.add(Dense(128,activation='swish'))\n",
        "model6.add(Dropout(0.2))\n",
        "# model6.add(Dense(64,activation='swish'))\n",
        "# model6.add(Dropout(0.2))\n",
        "model6.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model6.summary()\n",
        "model6.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "hist6=model6.fit(train_generator,epochs=50,validation_data=(validation_generator), batch_size=128, verbose=1)\n",
        "print(\"Model Training Complete...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist6=model6.fit(train_generator,epochs=20,validation_data=(validation_generator), batch_size=128, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiSHtyPrDrSV",
        "outputId": "af6382ed-c8e1-4fae-c431-780e8091c0c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "176/176 [==============================] - 249s 1s/step - loss: 0.2991 - accuracy: 0.9027 - val_loss: 0.2113 - val_accuracy: 0.9350\n",
            "Epoch 2/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.3040 - accuracy: 0.8985 - val_loss: 0.2007 - val_accuracy: 0.9377\n",
            "Epoch 3/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2929 - accuracy: 0.9040 - val_loss: 0.1960 - val_accuracy: 0.9369\n",
            "Epoch 4/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2886 - accuracy: 0.9047 - val_loss: 0.1898 - val_accuracy: 0.9411\n",
            "Epoch 5/20\n",
            "176/176 [==============================] - 246s 1s/step - loss: 0.2889 - accuracy: 0.9066 - val_loss: 0.1927 - val_accuracy: 0.9399\n",
            "Epoch 6/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2701 - accuracy: 0.9113 - val_loss: 0.1774 - val_accuracy: 0.9461\n",
            "Epoch 7/20\n",
            "176/176 [==============================] - 248s 1s/step - loss: 0.2670 - accuracy: 0.9135 - val_loss: 0.1790 - val_accuracy: 0.9463\n",
            "Epoch 8/20\n",
            "176/176 [==============================] - 248s 1s/step - loss: 0.2588 - accuracy: 0.9156 - val_loss: 0.1902 - val_accuracy: 0.9418\n",
            "Epoch 9/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2559 - accuracy: 0.9169 - val_loss: 0.1706 - val_accuracy: 0.9464\n",
            "Epoch 10/20\n",
            "176/176 [==============================] - 248s 1s/step - loss: 0.2488 - accuracy: 0.9201 - val_loss: 0.1802 - val_accuracy: 0.9444\n",
            "Epoch 11/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2481 - accuracy: 0.9197 - val_loss: 0.1862 - val_accuracy: 0.9429\n",
            "Epoch 12/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2476 - accuracy: 0.9210 - val_loss: 0.1580 - val_accuracy: 0.9512\n",
            "Epoch 13/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2354 - accuracy: 0.9241 - val_loss: 0.1509 - val_accuracy: 0.9534\n",
            "Epoch 14/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2327 - accuracy: 0.9251 - val_loss: 0.1688 - val_accuracy: 0.9473\n",
            "Epoch 15/20\n",
            "176/176 [==============================] - 249s 1s/step - loss: 0.2280 - accuracy: 0.9255 - val_loss: 0.1459 - val_accuracy: 0.9560\n",
            "Epoch 16/20\n",
            "176/176 [==============================] - 248s 1s/step - loss: 0.2202 - accuracy: 0.9287 - val_loss: 0.1532 - val_accuracy: 0.9497\n",
            "Epoch 17/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2192 - accuracy: 0.9304 - val_loss: 0.1425 - val_accuracy: 0.9552\n",
            "Epoch 18/20\n",
            "176/176 [==============================] - 248s 1s/step - loss: 0.2194 - accuracy: 0.9281 - val_loss: 0.1421 - val_accuracy: 0.9567\n",
            "Epoch 19/20\n",
            "176/176 [==============================] - 248s 1s/step - loss: 0.2112 - accuracy: 0.9326 - val_loss: 0.1335 - val_accuracy: 0.9585\n",
            "Epoch 20/20\n",
            "176/176 [==============================] - 247s 1s/step - loss: 0.2077 - accuracy: 0.9327 - val_loss: 0.1327 - val_accuracy: 0.9592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "my_img = plt.imread(\"img1.jpg\")\n",
        "\n",
        "my_img_resized = resize(my_img,(160,160,3))\n",
        "\n",
        "probabilities = model6.predict(np.array([my_img_resized,]))\n",
        "number_to_class = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities[0,:])\n",
        "print(\"Image 1:\", number_to_class[index[9]], \"-- Possibility:\", probabilities[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 2\n",
        "my_img2 = plt.imread(\"img2.jpg\")\n",
        "my_img2_resized = resize(my_img2,(160,160,3))\n",
        "\n",
        "probabilities2 = model6.predict(np.array([my_img2_resized,]))\n",
        "number_to_class2 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities2[0,:])\n",
        "print(\"Image 2:\", number_to_class2[index[9]], \"-- Possibility:\", probabilities2[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "# image 3\n",
        "my_img3 = plt.imread(\"img3.jpg\")\n",
        "my_img3_resized = resize(my_img3,(160,160,3))\n",
        "\n",
        "probabilities3 = model6.predict(np.array([my_img3_resized,]))\n",
        "number_to_class3 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities3[0,:])\n",
        "print(\"Image 3:\", number_to_class3[index[9]], \"-- Possibility:\", probabilities3[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "# image 4\n",
        "my_img4 = plt.imread(\"img4.jpg\")\n",
        "my_img4_resized = resize(my_img4,(160,160,3))\n",
        "\n",
        "probabilities4 = model6.predict(np.array([my_img4_resized,]))\n",
        "number_to_class4 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities4[0,:])\n",
        "print(\"Image 4:\", number_to_class4[index[9]], \"-- Possibility:\", probabilities4[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 5\n",
        "my_img5 = plt.imread(\"img5.jpg\")\n",
        "my_img5_resized = resize(my_img5,(160,160,3))\n",
        "\n",
        "probabilities5 = model6.predict(np.array([my_img5_resized,]))\n",
        "number_to_class5 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities5[0,:])\n",
        "print(\"Image 5:\", number_to_class5[index[9]], \"-- Possibility:\", probabilities5[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 6\n",
        "my_img6 = plt.imread(\"img6.jpg\")\n",
        "my_img6_resized = resize(my_img6,(160,160,3))\n",
        "\n",
        "probabilities6 = model6.predict(np.array([my_img6_resized,]))\n",
        "number_to_class6 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities6[0,:])\n",
        "print(\"Image 6:\", number_to_class6[index[9]], \"-- Possibility:\", probabilities6[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 7\n",
        "my_img7 = plt.imread(\"img7.jpg\")\n",
        "my_img7_resized = resize(my_img7,(160,160,3))\n",
        "\n",
        "probabilities7 = model6.predict(np.array([my_img7_resized,]))\n",
        "number_to_class7 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities7[0,:])\n",
        "print(\"Image 7:\", number_to_class7[index[9]], \"-- Possibility:\", probabilities7[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 8\n",
        "my_img8 = plt.imread(\"img8.jpg\")\n",
        "my_img8_resized = resize(my_img8,(160,160,3))\n",
        "\n",
        "probabilities8 = model6.predict(np.array([my_img8_resized,]))\n",
        "number_to_class8 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities8[0,:])\n",
        "print(\"Image 8:\", number_to_class8[index[9]], \"-- Possibility:\", probabilities8[0,index[9]]*100, \"%\")\n",
        "\n",
        "# image 9\n",
        "my_img9 = plt.imread(\"img9.jpg\")\n",
        "my_img9_resized = resize(my_img9,(160,160,3))\n",
        "\n",
        "probabilities9 = model6.predict(np.array([my_img9_resized,]))\n",
        "number_to_class9 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities9[0,:])\n",
        "print(\"Image 9:\", number_to_class9[index[9]], \"-- Possibility:\", probabilities9[0,index[9]]*100, \"%\")\n",
        "\n",
        "# image 10\n",
        "my_img10 = plt.imread(\"img10.jpg\")\n",
        "my_img10_resized = resize(my_img10,(160,160,3))\n",
        "\n",
        "probabilities10 = model6.predict(np.array([my_img10_resized,]))\n",
        "number_to_class10 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities10[0,:])\n",
        "print(\"Image 10:\", number_to_class10[index[9]], \"-- Possibility:\", probabilities10[0,index[9]]*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCVzYSzeBoDO",
        "outputId": "cf5b68dd-b59a-422e-f04a-ec13608c561b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1: Operating the radio -- Possibility: 99.97984766960144 %\n",
            "Image 2: Talking on the phone - right -- Possibility: 55.51528334617615 %\n",
            "Image 3: Talking on the phone - left -- Possibility: 99.30364489555359 %\n",
            "Image 4: Talking on the phone - right -- Possibility: 48.75800609588623 %\n",
            "Image 5: Texting - left -- Possibility: 99.35235977172852 %\n",
            "Image 6: Texting - right -- Possibility: 99.91005659103394 %\n",
            "Image 7: Texting - right -- Possibility: 85.45993566513062 %\n",
            "Image 8: Texting - left -- Possibility: 96.62460684776306 %\n",
            "Image 9: Operating the radio -- Possibility: 94.31190490722656 %\n",
            "Image 10: Operating the radio -- Possibility: 99.7848629951477 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "my_img = plt.imread(\"img1.jpg\")\n",
        "\n",
        "my_img_resized = resize(my_img,(160,160,3))\n",
        "\n",
        "probabilities = model6.predict(np.array([my_img_resized,]))\n",
        "number_to_class = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities[0,:])\n",
        "print(\"Image 1:\", number_to_class[index[9]], \"-- Possibility:\", probabilities[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 2\n",
        "my_img2 = plt.imread(\"img2.jpg\")\n",
        "my_img2_resized = resize(my_img2,(160,160,3))\n",
        "\n",
        "probabilities2 = model6.predict(np.array([my_img2_resized,]))\n",
        "number_to_class2 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities2[0,:])\n",
        "print(\"Image 2:\", number_to_class2[index[9]], \"-- Possibility:\", probabilities2[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "# image 3\n",
        "my_img3 = plt.imread(\"img3.jpg\")\n",
        "my_img3_resized = resize(my_img3,(160,160,3))\n",
        "\n",
        "probabilities3 = model6.predict(np.array([my_img3_resized,]))\n",
        "number_to_class3 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities3[0,:])\n",
        "print(\"Image 3:\", number_to_class3[index[9]], \"-- Possibility:\", probabilities3[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "# image 4\n",
        "my_img4 = plt.imread(\"img4.jpg\")\n",
        "my_img4_resized = resize(my_img4,(160,160,3))\n",
        "\n",
        "probabilities4 = model6.predict(np.array([my_img4_resized,]))\n",
        "number_to_class4 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities4[0,:])\n",
        "print(\"Image 4:\", number_to_class4[index[9]], \"-- Possibility:\", probabilities4[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 5\n",
        "my_img5 = plt.imread(\"img5.jpg\")\n",
        "my_img5_resized = resize(my_img5,(160,160,3))\n",
        "\n",
        "probabilities5 = model6.predict(np.array([my_img5_resized,]))\n",
        "number_to_class5 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities5[0,:])\n",
        "print(\"Image 5:\", number_to_class5[index[9]], \"-- Possibility:\", probabilities5[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 6\n",
        "my_img6 = plt.imread(\"img6.jpg\")\n",
        "my_img6_resized = resize(my_img6,(160,160,3))\n",
        "\n",
        "probabilities6 = model6.predict(np.array([my_img6_resized,]))\n",
        "number_to_class6 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities6[0,:])\n",
        "print(\"Image 6:\", number_to_class6[index[9]], \"-- Possibility:\", probabilities6[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 7\n",
        "my_img7 = plt.imread(\"img7.jpg\")\n",
        "my_img7_resized = resize(my_img7,(160,160,3))\n",
        "\n",
        "probabilities7 = model6.predict(np.array([my_img7_resized,]))\n",
        "number_to_class7 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities7[0,:])\n",
        "print(\"Image 7:\", number_to_class7[index[9]], \"-- Possibility:\", probabilities7[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 8\n",
        "my_img8 = plt.imread(\"img8.jpg\")\n",
        "my_img8_resized = resize(my_img8,(160,160,3))\n",
        "\n",
        "probabilities8 = model6.predict(np.array([my_img8_resized,]))\n",
        "number_to_class8 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities8[0,:])\n",
        "print(\"Image 8:\", number_to_class8[index[9]], \"-- Possibility:\", probabilities8[0,index[9]]*100, \"%\")\n",
        "\n",
        "# image 9\n",
        "my_img9 = plt.imread(\"img9.jpg\")\n",
        "my_img9_resized = resize(my_img9,(160,160,3))\n",
        "\n",
        "probabilities9 = model6.predict(np.array([my_img9_resized,]))\n",
        "number_to_class9 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities9[0,:])\n",
        "print(\"Image 9:\", number_to_class9[index[9]], \"-- Possibility:\", probabilities9[0,index[9]]*100, \"%\")\n",
        "\n",
        "# image 10\n",
        "my_img10 = plt.imread(\"img10.jpg\")\n",
        "my_img10_resized = resize(my_img10,(160,160,3))\n",
        "\n",
        "probabilities10 = model6.predict(np.array([my_img10_resized,]))\n",
        "number_to_class10 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities10[0,:])\n",
        "print(\"Image 10:\", number_to_class10[index[9]], \"-- Possibility:\", probabilities10[0,index[9]]*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrsrV-6mXRij",
        "outputId": "56779a87-411b-4461-a7e6-67c49c6492d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1: Operating the radio -- Possibility: 99.99854564666748 %\n",
            "Image 2: Talking on the phone - right -- Possibility: 78.96634936332703 %\n",
            "Image 3: Talking on the phone - left -- Possibility: 99.79065656661987 %\n",
            "Image 4: Talking on the phone - right -- Possibility: 90.80380201339722 %\n",
            "Image 5: Texting - left -- Possibility: 98.76547455787659 %\n",
            "Image 6: Texting - right -- Possibility: 99.99204874038696 %\n",
            "Image 7: Texting - right -- Possibility: 86.02083921432495 %\n",
            "Image 8: Texting - left -- Possibility: 98.12450408935547 %\n",
            "Image 9: Operating the radio -- Possibility: 98.7622857093811 %\n",
            "Image 10: Operating the radio -- Possibility: 99.48786497116089 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptoD18gBQpzZ"
      },
      "outputs": [],
      "source": [
        "model10 = Sequential() \n",
        "\n",
        "\n",
        "# first layer of the neural network. \n",
        "model10.add(Conv2D(32,(3,3), activation = 'relu', padding = 'same', input_shape = (160,160,3)))\n",
        "# second layer of the neural network in this we dont need to specify input shape as it will take input form the previous layer\n",
        "# model10.add(Conv2D(32,(3,3), activation = 'relu', padding = 'same')) \n",
        "model10.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model10.add(Dropout(0.2))\n",
        "model10.add(Conv2D(64,(3,3), activation = 'relu', padding = 'same'))\n",
        "# model10.add(Conv2D(64,(3,3), activation = 'relu', padding = 'same'))\n",
        "model10.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model10.add(Dropout(0.2))\n",
        "model10.add(Conv2D(128,(3,3), activation = 'relu', padding = 'same'))\n",
        "model10.add(Conv2D(128,(3,3), activation = 'relu', padding = 'same'))\n",
        "model10.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model10.add(Flatten())\n",
        "model10.add(Dense(128, activation = 'relu'))\n",
        "model10.add(Dropout(0.2))\n",
        "model10.add(Dense(64, activation = 'relu'))\n",
        "model10.add(Dropout(0.2))\n",
        "model10.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "\n",
        "model10.summary()\n",
        "model10.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "hist10=model10.fit(train_generator,epochs=1,validation_data=validation_generator, verbose=1)\n",
        "print(\"Model Training Complete...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbmwaRs4Qzwh"
      },
      "outputs": [],
      "source": [
        "ImplementingVGG16=tf.keras.applications.VGG16(input_shape=(160,160,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "mod1=ImplementingVGG16.output\n",
        "mod1=tf.keras.layers.Flatten()(mod1)\n",
        "mod1=tf.keras.layers.Dense(units=256, activation=tf.nn.relu)(mod1)\n",
        "mod1=tf.keras.layers.Dense(units=256, activation=tf.nn.relu)(mod1)\n",
        "output1=tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)(mod1)\n",
        "model1= tf.keras.models.Model(inputs=ImplementingVGG16.inputs,outputs=output1)\n",
        "\n",
        "model1.compile(optimizer=tf.keras.optimizers.Adam(0.00000005),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(from_logits= False),\n",
        "             metrics=['accuracy'])\n",
        "model1.summary()\n",
        "hist1=model1.fit(train_generator,epochs=50,validation_data=(validation_generator), verbose=1)\n",
        "print(\"Model Training Complete...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYrkYBJqQ5OJ"
      },
      "outputs": [],
      "source": [
        "ImplementingResNet=tf.keras.applications.resnet.ResNet50(include_top= False, \n",
        "                                                         weights='imagenet',\n",
        "                                                         input_shape= (input_shape_value))\n",
        "ImplementingResNet.summary()\n",
        "mod=ImplementingResNet.output\n",
        "mod=tf.keras.layers.Flatten()(mod)\n",
        "mod=tf.keras.layers.Dense(units=32, activation=tf.nn.relu)(mod)\n",
        "mod=tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(mod)\n",
        "# mod=tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(mod)\n",
        "# mod=tf.keras.layers.Dense(units=32, activation=tf.nn.relu)(mod)\n",
        "# mod=tf.keras.layers.Dense(units=16, activation=tf.nn.relu)(mod)\n",
        "output=tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)(mod)\n",
        "model8= tf.keras.models.Model(inputs=ImplementingResNet.inputs,outputs=output)\n",
        "\n",
        "model8.compile(optimizer=tf.keras.optimizers.Adam(0.0000005),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(from_logits= False),\n",
        "             metrics=['accuracy'])\n",
        "model8.summary()\n",
        "hist8=model8.fit(X_train,Y_train,epochs=20,validation_data=(X_test,Y_test), verbose=1)\n",
        "print(\"Model Training Complete...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DS7XqFjBRAFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e67962-e24e-408a-a554-ce4fc92af859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)                 (None, 64, 64, 32)   864         ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalization)  (None, 64, 64, 32)   128         ['Conv1[0][0]']                  \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)              (None, 64, 64, 32)   0           ['bn_Conv1[0][0]']               \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (Depth  (None, 64, 64, 32)  288         ['Conv1_relu[0][0]']             \n",
            " wiseConv2D)                                                                                      \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN (Ba  (None, 64, 64, 32)  128         ['expanded_conv_depthwise[0][0]']\n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_relu (  (None, 64, 64, 32)  0           ['expanded_conv_depthwise_BN[0][0\n",
            " ReLU)                                                           ]']                              \n",
            "                                                                                                  \n",
            " expanded_conv_project (Conv2D)  (None, 64, 64, 16)  512         ['expanded_conv_depthwise_relu[0]\n",
            "                                                                 [0]']                            \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (Batc  (None, 64, 64, 16)  64          ['expanded_conv_project[0][0]']  \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)        (None, 64, 64, 96)   1536        ['expanded_conv_project_BN[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNormal  (None, 64, 64, 96)  384         ['block_1_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)     (None, 64, 64, 96)   0           ['block_1_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D)    (None, 65, 65, 96)   0           ['block_1_expand_relu[0][0]']    \n",
            "                                                                                                  \n",
            " block_1_depthwise (DepthwiseCo  (None, 32, 32, 96)  864         ['block_1_pad[0][0]']            \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (BatchNor  (None, 32, 32, 96)  384         ['block_1_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (ReLU)  (None, 32, 32, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)       (None, 32, 32, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchNorma  (None, 32, 32, 24)  96          ['block_1_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)        (None, 32, 32, 144)  3456        ['block_1_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNormal  (None, 32, 32, 144)  576        ['block_2_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)     (None, 32, 32, 144)  0           ['block_2_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_2_depthwise (DepthwiseCo  (None, 32, 32, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (BatchNor  (None, 32, 32, 144)  576        ['block_2_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (ReLU)  (None, 32, 32, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)       (None, 32, 32, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchNorma  (None, 32, 32, 24)  96          ['block_2_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_add (Add)              (None, 32, 32, 24)   0           ['block_1_project_BN[0][0]',     \n",
            "                                                                  'block_2_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)        (None, 32, 32, 144)  3456        ['block_2_add[0][0]']            \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNormal  (None, 32, 32, 144)  576        ['block_3_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)     (None, 32, 32, 144)  0           ['block_3_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D)    (None, 33, 33, 144)  0           ['block_3_expand_relu[0][0]']    \n",
            "                                                                                                  \n",
            " block_3_depthwise (DepthwiseCo  (None, 16, 16, 144)  1296       ['block_3_pad[0][0]']            \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (BatchNor  (None, 16, 16, 144)  576        ['block_3_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (ReLU)  (None, 16, 16, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)       (None, 16, 16, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchNorma  (None, 16, 16, 32)  128         ['block_3_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)        (None, 16, 16, 192)  6144        ['block_3_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNormal  (None, 16, 16, 192)  768        ['block_4_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)     (None, 16, 16, 192)  0           ['block_4_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_4_depthwise (DepthwiseCo  (None, 16, 16, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (BatchNor  (None, 16, 16, 192)  768        ['block_4_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (ReLU)  (None, 16, 16, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)       (None, 16, 16, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchNorma  (None, 16, 16, 32)  128         ['block_4_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_add (Add)              (None, 16, 16, 32)   0           ['block_3_project_BN[0][0]',     \n",
            "                                                                  'block_4_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)        (None, 16, 16, 192)  6144        ['block_4_add[0][0]']            \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNormal  (None, 16, 16, 192)  768        ['block_5_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)     (None, 16, 16, 192)  0           ['block_5_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_5_depthwise (DepthwiseCo  (None, 16, 16, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (BatchNor  (None, 16, 16, 192)  768        ['block_5_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (ReLU)  (None, 16, 16, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)       (None, 16, 16, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchNorma  (None, 16, 16, 32)  128         ['block_5_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_add (Add)              (None, 16, 16, 32)   0           ['block_4_add[0][0]',            \n",
            "                                                                  'block_5_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)        (None, 16, 16, 192)  6144        ['block_5_add[0][0]']            \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNormal  (None, 16, 16, 192)  768        ['block_6_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)     (None, 16, 16, 192)  0           ['block_6_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_6_pad (ZeroPadding2D)    (None, 17, 17, 192)  0           ['block_6_expand_relu[0][0]']    \n",
            "                                                                                                  \n",
            " block_6_depthwise (DepthwiseCo  (None, 8, 8, 192)   1728        ['block_6_pad[0][0]']            \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_6_depthwise_BN (BatchNor  (None, 8, 8, 192)   768         ['block_6_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_6_depthwise_relu (ReLU)  (None, 8, 8, 192)    0           ['block_6_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_6_project (Conv2D)       (None, 8, 8, 64)     12288       ['block_6_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_6_project_BN (BatchNorma  (None, 8, 8, 64)    256         ['block_6_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_7_expand (Conv2D)        (None, 8, 8, 384)    24576       ['block_6_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_7_expand_BN (BatchNormal  (None, 8, 8, 384)   1536        ['block_7_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_7_expand_relu (ReLU)     (None, 8, 8, 384)    0           ['block_7_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_7_depthwise (DepthwiseCo  (None, 8, 8, 384)   3456        ['block_7_expand_relu[0][0]']    \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_7_depthwise_BN (BatchNor  (None, 8, 8, 384)   1536        ['block_7_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_7_depthwise_relu (ReLU)  (None, 8, 8, 384)    0           ['block_7_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_7_project (Conv2D)       (None, 8, 8, 64)     24576       ['block_7_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_7_project_BN (BatchNorma  (None, 8, 8, 64)    256         ['block_7_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_7_add (Add)              (None, 8, 8, 64)     0           ['block_6_project_BN[0][0]',     \n",
            "                                                                  'block_7_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_8_expand (Conv2D)        (None, 8, 8, 384)    24576       ['block_7_add[0][0]']            \n",
            "                                                                                                  \n",
            " block_8_expand_BN (BatchNormal  (None, 8, 8, 384)   1536        ['block_8_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_8_expand_relu (ReLU)     (None, 8, 8, 384)    0           ['block_8_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_8_depthwise (DepthwiseCo  (None, 8, 8, 384)   3456        ['block_8_expand_relu[0][0]']    \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_8_depthwise_BN (BatchNor  (None, 8, 8, 384)   1536        ['block_8_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_8_depthwise_relu (ReLU)  (None, 8, 8, 384)    0           ['block_8_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_8_project (Conv2D)       (None, 8, 8, 64)     24576       ['block_8_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_8_project_BN (BatchNorma  (None, 8, 8, 64)    256         ['block_8_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_8_add (Add)              (None, 8, 8, 64)     0           ['block_7_add[0][0]',            \n",
            "                                                                  'block_8_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_9_expand (Conv2D)        (None, 8, 8, 384)    24576       ['block_8_add[0][0]']            \n",
            "                                                                                                  \n",
            " block_9_expand_BN (BatchNormal  (None, 8, 8, 384)   1536        ['block_9_expand[0][0]']         \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " block_9_expand_relu (ReLU)     (None, 8, 8, 384)    0           ['block_9_expand_BN[0][0]']      \n",
            "                                                                                                  \n",
            " block_9_depthwise (DepthwiseCo  (None, 8, 8, 384)   3456        ['block_9_expand_relu[0][0]']    \n",
            " nv2D)                                                                                            \n",
            "                                                                                                  \n",
            " block_9_depthwise_BN (BatchNor  (None, 8, 8, 384)   1536        ['block_9_depthwise[0][0]']      \n",
            " malization)                                                                                      \n",
            "                                                                                                  \n",
            " block_9_depthwise_relu (ReLU)  (None, 8, 8, 384)    0           ['block_9_depthwise_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_9_project (Conv2D)       (None, 8, 8, 64)     24576       ['block_9_depthwise_relu[0][0]'] \n",
            "                                                                                                  \n",
            " block_9_project_BN (BatchNorma  (None, 8, 8, 64)    256         ['block_9_project[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_9_add (Add)              (None, 8, 8, 64)     0           ['block_8_add[0][0]',            \n",
            "                                                                  'block_9_project_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_10_expand (Conv2D)       (None, 8, 8, 384)    24576       ['block_9_add[0][0]']            \n",
            "                                                                                                  \n",
            " block_10_expand_BN (BatchNorma  (None, 8, 8, 384)   1536        ['block_10_expand[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_10_expand_relu (ReLU)    (None, 8, 8, 384)    0           ['block_10_expand_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_10_depthwise (DepthwiseC  (None, 8, 8, 384)   3456        ['block_10_expand_relu[0][0]']   \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " block_10_depthwise_BN (BatchNo  (None, 8, 8, 384)   1536        ['block_10_depthwise[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_10_depthwise_relu (ReLU)  (None, 8, 8, 384)   0           ['block_10_depthwise_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_10_project (Conv2D)      (None, 8, 8, 96)     36864       ['block_10_depthwise_relu[0][0]']\n",
            "                                                                                                  \n",
            " block_10_project_BN (BatchNorm  (None, 8, 8, 96)    384         ['block_10_project[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block_11_expand (Conv2D)       (None, 8, 8, 576)    55296       ['block_10_project_BN[0][0]']    \n",
            "                                                                                                  \n",
            " block_11_expand_BN (BatchNorma  (None, 8, 8, 576)   2304        ['block_11_expand[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_11_expand_relu (ReLU)    (None, 8, 8, 576)    0           ['block_11_expand_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_11_depthwise (DepthwiseC  (None, 8, 8, 576)   5184        ['block_11_expand_relu[0][0]']   \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " block_11_depthwise_BN (BatchNo  (None, 8, 8, 576)   2304        ['block_11_depthwise[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_11_depthwise_relu (ReLU)  (None, 8, 8, 576)   0           ['block_11_depthwise_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_11_project (Conv2D)      (None, 8, 8, 96)     55296       ['block_11_depthwise_relu[0][0]']\n",
            "                                                                                                  \n",
            " block_11_project_BN (BatchNorm  (None, 8, 8, 96)    384         ['block_11_project[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block_11_add (Add)             (None, 8, 8, 96)     0           ['block_10_project_BN[0][0]',    \n",
            "                                                                  'block_11_project_BN[0][0]']    \n",
            "                                                                                                  \n",
            " block_12_expand (Conv2D)       (None, 8, 8, 576)    55296       ['block_11_add[0][0]']           \n",
            "                                                                                                  \n",
            " block_12_expand_BN (BatchNorma  (None, 8, 8, 576)   2304        ['block_12_expand[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_12_expand_relu (ReLU)    (None, 8, 8, 576)    0           ['block_12_expand_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_12_depthwise (DepthwiseC  (None, 8, 8, 576)   5184        ['block_12_expand_relu[0][0]']   \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " block_12_depthwise_BN (BatchNo  (None, 8, 8, 576)   2304        ['block_12_depthwise[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_12_depthwise_relu (ReLU)  (None, 8, 8, 576)   0           ['block_12_depthwise_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_12_project (Conv2D)      (None, 8, 8, 96)     55296       ['block_12_depthwise_relu[0][0]']\n",
            "                                                                                                  \n",
            " block_12_project_BN (BatchNorm  (None, 8, 8, 96)    384         ['block_12_project[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block_12_add (Add)             (None, 8, 8, 96)     0           ['block_11_add[0][0]',           \n",
            "                                                                  'block_12_project_BN[0][0]']    \n",
            "                                                                                                  \n",
            " block_13_expand (Conv2D)       (None, 8, 8, 576)    55296       ['block_12_add[0][0]']           \n",
            "                                                                                                  \n",
            " block_13_expand_BN (BatchNorma  (None, 8, 8, 576)   2304        ['block_13_expand[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_13_expand_relu (ReLU)    (None, 8, 8, 576)    0           ['block_13_expand_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_13_pad (ZeroPadding2D)   (None, 9, 9, 576)    0           ['block_13_expand_relu[0][0]']   \n",
            "                                                                                                  \n",
            " block_13_depthwise (DepthwiseC  (None, 4, 4, 576)   5184        ['block_13_pad[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " block_13_depthwise_BN (BatchNo  (None, 4, 4, 576)   2304        ['block_13_depthwise[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_13_depthwise_relu (ReLU)  (None, 4, 4, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_13_project (Conv2D)      (None, 4, 4, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
            "                                                                                                  \n",
            " block_13_project_BN (BatchNorm  (None, 4, 4, 160)   640         ['block_13_project[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block_14_expand (Conv2D)       (None, 4, 4, 960)    153600      ['block_13_project_BN[0][0]']    \n",
            "                                                                                                  \n",
            " block_14_expand_BN (BatchNorma  (None, 4, 4, 960)   3840        ['block_14_expand[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_14_expand_relu (ReLU)    (None, 4, 4, 960)    0           ['block_14_expand_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_14_depthwise (DepthwiseC  (None, 4, 4, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " block_14_depthwise_BN (BatchNo  (None, 4, 4, 960)   3840        ['block_14_depthwise[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_14_depthwise_relu (ReLU)  (None, 4, 4, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_14_project (Conv2D)      (None, 4, 4, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
            "                                                                                                  \n",
            " block_14_project_BN (BatchNorm  (None, 4, 4, 160)   640         ['block_14_project[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block_14_add (Add)             (None, 4, 4, 160)    0           ['block_13_project_BN[0][0]',    \n",
            "                                                                  'block_14_project_BN[0][0]']    \n",
            "                                                                                                  \n",
            " block_15_expand (Conv2D)       (None, 4, 4, 960)    153600      ['block_14_add[0][0]']           \n",
            "                                                                                                  \n",
            " block_15_expand_BN (BatchNorma  (None, 4, 4, 960)   3840        ['block_15_expand[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_15_expand_relu (ReLU)    (None, 4, 4, 960)    0           ['block_15_expand_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_15_depthwise (DepthwiseC  (None, 4, 4, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " block_15_depthwise_BN (BatchNo  (None, 4, 4, 960)   3840        ['block_15_depthwise[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_15_depthwise_relu (ReLU)  (None, 4, 4, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_15_project (Conv2D)      (None, 4, 4, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
            "                                                                                                  \n",
            " block_15_project_BN (BatchNorm  (None, 4, 4, 160)   640         ['block_15_project[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block_15_add (Add)             (None, 4, 4, 160)    0           ['block_14_add[0][0]',           \n",
            "                                                                  'block_15_project_BN[0][0]']    \n",
            "                                                                                                  \n",
            " block_16_expand (Conv2D)       (None, 4, 4, 960)    153600      ['block_15_add[0][0]']           \n",
            "                                                                                                  \n",
            " block_16_expand_BN (BatchNorma  (None, 4, 4, 960)   3840        ['block_16_expand[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block_16_expand_relu (ReLU)    (None, 4, 4, 960)    0           ['block_16_expand_BN[0][0]']     \n",
            "                                                                                                  \n",
            " block_16_depthwise (DepthwiseC  (None, 4, 4, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " block_16_depthwise_BN (BatchNo  (None, 4, 4, 960)   3840        ['block_16_depthwise[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_16_depthwise_relu (ReLU)  (None, 4, 4, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_16_project (Conv2D)      (None, 4, 4, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
            "                                                                                                  \n",
            " block_16_project_BN (BatchNorm  (None, 4, 4, 320)   1280        ['block_16_project[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " Conv_1 (Conv2D)                (None, 4, 4, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
            "                                                                                                  \n",
            " Conv_1_bn (BatchNormalization)  (None, 4, 4, 1280)  5120        ['Conv_1[0][0]']                 \n",
            "                                                                                                  \n",
            " out_relu (ReLU)                (None, 4, 4, 1280)   0           ['Conv_1_bn[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 20480)        0           ['out_relu[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 32)           655392      ['flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 10)           330         ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,913,706\n",
            "Trainable params: 2,879,594\n",
            "Non-trainable params: 34,112\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "176/176 [==============================] - 156s 862ms/step - loss: 2.1112 - accuracy: 0.2233\n",
            "Epoch 2/30\n",
            "176/176 [==============================] - 153s 867ms/step - loss: 1.0976 - accuracy: 0.6196\n",
            "Epoch 3/30\n",
            "176/176 [==============================] - 153s 867ms/step - loss: 0.5534 - accuracy: 0.8171\n",
            "Epoch 4/30\n",
            "176/176 [==============================] - 152s 864ms/step - loss: 0.3486 - accuracy: 0.8870\n",
            "Epoch 5/30\n",
            "176/176 [==============================] - 152s 864ms/step - loss: 0.2710 - accuracy: 0.9102\n",
            "Epoch 6/30\n",
            "176/176 [==============================] - 152s 862ms/step - loss: 0.2174 - accuracy: 0.9309\n",
            "Epoch 7/30\n",
            "176/176 [==============================] - 152s 863ms/step - loss: 0.1835 - accuracy: 0.9421\n",
            "Epoch 8/30\n",
            "176/176 [==============================] - 152s 864ms/step - loss: 0.1593 - accuracy: 0.9478\n",
            "Epoch 9/30\n",
            "176/176 [==============================] - 153s 867ms/step - loss: 0.1343 - accuracy: 0.9576\n",
            "Epoch 10/30\n",
            "176/176 [==============================] - 152s 864ms/step - loss: 0.1211 - accuracy: 0.9625\n",
            "Epoch 11/30\n",
            "176/176 [==============================] - 153s 866ms/step - loss: 0.1059 - accuracy: 0.9663\n",
            "Epoch 12/30\n",
            "176/176 [==============================] - 152s 865ms/step - loss: 0.0979 - accuracy: 0.9698\n",
            "Epoch 13/30\n",
            "176/176 [==============================] - 153s 867ms/step - loss: 0.0928 - accuracy: 0.9699\n",
            "Epoch 14/30\n",
            "176/176 [==============================] - 153s 866ms/step - loss: 0.0802 - accuracy: 0.9732\n",
            "Epoch 15/30\n",
            "176/176 [==============================] - 152s 865ms/step - loss: 0.0827 - accuracy: 0.9736\n",
            "Epoch 16/30\n",
            "176/176 [==============================] - 153s 869ms/step - loss: 0.0695 - accuracy: 0.9767\n",
            "Epoch 17/30\n",
            "176/176 [==============================] - 153s 869ms/step - loss: 0.0696 - accuracy: 0.9773\n",
            "Epoch 18/30\n",
            "176/176 [==============================] - 153s 869ms/step - loss: 0.0648 - accuracy: 0.9787\n",
            "Epoch 19/30\n",
            "176/176 [==============================] - 153s 867ms/step - loss: 0.0605 - accuracy: 0.9806\n",
            "Epoch 20/30\n",
            "176/176 [==============================] - 153s 868ms/step - loss: 0.0620 - accuracy: 0.9802\n",
            "Epoch 21/30\n",
            "176/176 [==============================] - 152s 865ms/step - loss: 0.0584 - accuracy: 0.9814\n",
            "Epoch 22/30\n",
            "176/176 [==============================] - 153s 870ms/step - loss: 0.0579 - accuracy: 0.9808\n",
            "Epoch 23/30\n",
            "176/176 [==============================] - 153s 866ms/step - loss: 0.0537 - accuracy: 0.9824\n",
            "Epoch 24/30\n",
            "176/176 [==============================] - 153s 865ms/step - loss: 0.0529 - accuracy: 0.9831\n",
            "Epoch 25/30\n",
            "176/176 [==============================] - 153s 867ms/step - loss: 0.0578 - accuracy: 0.9805\n",
            "Epoch 26/30\n",
            "176/176 [==============================] - 152s 865ms/step - loss: 0.0438 - accuracy: 0.9863\n",
            "Epoch 27/30\n",
            "176/176 [==============================] - 153s 868ms/step - loss: 0.0398 - accuracy: 0.9867\n",
            "Epoch 28/30\n",
            "176/176 [==============================] - 153s 866ms/step - loss: 0.0434 - accuracy: 0.9855\n",
            "Epoch 29/30\n",
            "176/176 [==============================] - 152s 866ms/step - loss: 0.0469 - accuracy: 0.9859\n",
            "Epoch 30/30\n",
            "176/176 [==============================] - 153s 866ms/step - loss: 0.0480 - accuracy: 0.9854\n",
            "Model Training Complete...\n",
            "53/53 [==============================] - 46s 862ms/step - loss: 0.1971 - accuracy: 0.9432\n",
            "accuracy: 94.32%\n"
          ]
        }
      ],
      "source": [
        "ImplementingInception=tf.keras.applications.MobileNetV2(input_shape=(128,128,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "mod2=ImplementingInception.output\n",
        "mod2=tf.keras.layers.Flatten()(mod2)\n",
        "mod2=tf.keras.layers.Dense(units=32, activation=tf.nn.relu)(mod2)\n",
        "output2=tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)(mod2)\n",
        "model2= tf.keras.models.Model(inputs=ImplementingInception.inputs,outputs=output2)\n",
        "\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(from_logits= False),\n",
        "             metrics=['accuracy'])\n",
        "model2.summary()\n",
        "history=model2.fit(train_generator,epochs=30,verbose=1)\n",
        "print(\"Model Training Complete...\")\n",
        "(loss, accuracy) =model2.evaluate(validation_generator,batch_size=128, verbose=1)\n",
        "print(\"accuracy: {:.2f}%\".format(accuracy*100))\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.save('incep.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzBmVOJz48Nq",
        "outputId": "19e0d3d6-7145-4ef7-c748-cc57fe78bc77"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "my_img = plt.imread(\"img1.jpg\")\n",
        "\n",
        "my_img_resized = resize(my_img,(160,160,3))\n",
        "\n",
        "probabilities = model6.predict(np.array([my_img_resized,]))\n",
        "number_to_class = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities[0,:])\n",
        "print(\"Image 1:\", number_to_class[index[9]], \"-- Possibility:\", probabilities[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 2\n",
        "my_img2 = plt.imread(\"img2.jpg\")\n",
        "my_img2_resized = resize(my_img2,(160,160,3))\n",
        "\n",
        "probabilities2 = model6.predict(np.array([my_img2_resized,]))\n",
        "number_to_class2 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities2[0,:])\n",
        "print(\"Image 2:\", number_to_class2[index[9]], \"-- Possibility:\", probabilities2[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "# image 3\n",
        "my_img3 = plt.imread(\"img3.jpg\")\n",
        "my_img3_resized = resize(my_img3,(160,160,3))\n",
        "\n",
        "probabilities3 = model6.predict(np.array([my_img3_resized,]))\n",
        "number_to_class3 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities3[0,:])\n",
        "print(\"Image 3:\", number_to_class3[index[9]], \"-- Possibility:\", probabilities3[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "\n",
        "# image 4\n",
        "my_img4 = plt.imread(\"img4.jpg\")\n",
        "my_img4_resized = resize(my_img4,(160,160,3))\n",
        "\n",
        "probabilities4 = model6.predict(np.array([my_img4_resized,]))\n",
        "number_to_class4 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities4[0,:])\n",
        "print(\"Image 4:\", number_to_class4[index[9]], \"-- Possibility:\", probabilities4[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 5\n",
        "my_img5 = plt.imread(\"img5.jpg\")\n",
        "my_img5_resized = resize(my_img5,(160,160,3))\n",
        "\n",
        "probabilities5 = model6.predict(np.array([my_img5_resized,]))\n",
        "number_to_class5 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities5[0,:])\n",
        "print(\"Image 5:\", number_to_class5[index[9]], \"-- Possibility:\", probabilities5[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 6\n",
        "my_img6 = plt.imread(\"img6.jpg\")\n",
        "my_img6_resized = resize(my_img6,(160,160,3))\n",
        "\n",
        "probabilities6 = model6.predict(np.array([my_img6_resized,]))\n",
        "number_to_class6 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities6[0,:])\n",
        "print(\"Image 6:\", number_to_class6[index[9]], \"-- Possibility:\", probabilities6[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 7\n",
        "my_img7 = plt.imread(\"img7.jpg\")\n",
        "my_img7_resized = resize(my_img7,(160,160,3))\n",
        "\n",
        "probabilities7 = model6.predict(np.array([my_img7_resized,]))\n",
        "number_to_class7 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities7[0,:])\n",
        "print(\"Image 7:\", number_to_class7[index[9]], \"-- Possibility:\", probabilities7[0,index[9]]*100, \"%\")\n",
        "\n",
        "\n",
        "# image 8\n",
        "my_img8 = plt.imread(\"img8.jpg\")\n",
        "my_img8_resized = resize(my_img8,(160,160,3))\n",
        "\n",
        "probabilities8 = model6.predict(np.array([my_img8_resized,]))\n",
        "number_to_class8 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities8[0,:])\n",
        "print(\"Image 8:\", number_to_class8[index[9]], \"-- Possibility:\", probabilities8[0,index[9]]*100, \"%\")\n",
        "\n",
        "# image 9\n",
        "my_img9 = plt.imread(\"img9.jpg\")\n",
        "my_img9_resized = resize(my_img9,(160,160,3))\n",
        "\n",
        "probabilities9 = model6.predict(np.array([my_img9_resized,]))\n",
        "number_to_class9 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities9[0,:])\n",
        "print(\"Image 9:\", number_to_class9[index[9]], \"-- Possibility:\", probabilities9[0,index[9]]*100, \"%\")\n",
        "\n",
        "# image 10\n",
        "my_img10 = plt.imread(\"img10.jpg\")\n",
        "my_img10_resized = resize(my_img10,(160,160,3))\n",
        "\n",
        "probabilities10 = model6.predict(np.array([my_img10_resized,]))\n",
        "number_to_class10 = ['Safe driving','Texting - right', 'Talking on the phone - right', 'Texting - left', 'Talking on the phone - left', 'Operating the radio', 'Drinking', 'Reaching behind', 'Hair and makeup', 'Talking to passenger']\n",
        "\n",
        "index = np.argsort(probabilities10[0,:])\n",
        "print(\"Image 10:\", number_to_class10[index[9]], \"-- Possibility:\", probabilities10[0,index[9]]*100, \"%\")"
      ],
      "metadata": {
        "id": "rTr4kuyX4PrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results:\n",
        ">Model name\tTest accuracy\tNumber of epochs\tNumber of hyperparameters\tActivation function\tOptimizers\tFilters\n",
        "\t\t\t\t\t\t\n",
        "Resnet 50\t97.61\t10\t23653610\tReLu\tAdam\tNA\n",
        "Vgg16\t97.44\t10\t18060106\tReLu\tAdam\tNA\n",
        "Inception v3\t96.95\t30\t2913706\tReLu\tAdam\tNA\n",
        "CNN 6 layer\t98.01\t50\t201804618\tSwish\tSgd\t16X16\n",
        "CNN 5 Layer\t98\t100\t118799690\tSwish\tSgd\t8X8\n",
        "CNN 4 Layer\t73\t50\t16861898\tReLu\tAdam\t3X3\n",
        "\n",
        "\n",
        "As it is all the models have been tried with a lot of different hyperparameters that are different numbers of nodes in each layer, each model has been trained for different filter sizes, all the optimizers and activation functions have been applied and the best giving results have been presented here. This report suggests a CNN model of 6 layers as it gives 98 % accuracy and when given 10 random images it predicts 9 on 10 correctly with more than 95% possibility for each category. As compared to other models this model returns the best predictions on random images.\n",
        "The following images are provided in this sequence to all the models for prediction. The prediction of all 10 pictures from CNN-6 (the recommended model) has been provided below for verification purposes.  For all other models, h5 files have been provided along with the data for comparison and verification. Due to the limited amount of time and resources, these were the best results that were acquired. All the models have been trained on Google colab pro and hence only an ipynb file can be provided. For better results, there can be more work done on pre-trained models as they drastically reduce the number of hyperparameters and train quite fast. As in this project, Google Colab provides GPU to work with it was easier to train with 201 million hyperparameters too. If fewer resources are available to anyone trying to replicate this model, it is recommended to use 64X64 pixel images that can provide the accuracy but feature extraction isn’t that good. So, the results might be a bit faulty. If the resources are adequate it is recommended to use 256X256 pixel images.\n"
      ],
      "metadata": {
        "id": "UhOhEGqDZHTi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "split divided.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtzL3WH2iAHKloDJkQO0/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}